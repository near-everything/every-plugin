---
title: Data Aggregation Pipeline
description: Build a complete data aggregation pipeline using multiple source plugins and transformers
---

## Overview

This example demonstrates building a data aggregation pipeline that combines multiple data sources, processes the data through transformers, and outputs structured results. We'll create a news aggregation system that pulls from multiple sources and standardizes the output.

<Mermaid
  chart="
graph LR
    A[Twitter Source] --> D[Content Normalizer]
    B[Reddit Source] --> D
    C[RSS Source] --> D
    
    D --> E[Sentiment Analyzer]
    E --> F[Category Classifier]
    F --> G[Duplicate Detector]
    G --> H[Final Results]
    
    classDef sources fill:#2563eb,stroke:#1d4ed8,stroke-width:2px,color:#fff
    classDef transformers fill:#7c3aed,stroke:#6d28d9,stroke-width:2px,color:#fff
    classDef results fill:#059669,stroke:#047857,stroke-width:2px,color:#fff
    
    class A,B,C sources
    class D,E,F,G transformers
    class H results"
/>

## Setup

First, create the runtime with multiple plugins:

```typescript
import { Effect } from "effect";
import { createPluginRuntime, PluginRuntime } from "every-plugin/runtime";

const runtime = createPluginRuntime({
  registry: {
    "@sources/twitter": {
      remoteUrl: "https://cdn.example.com/plugins/twitter/remoteEntry.js",
      type: "source",
      version: "1.2.0",
    },
    "@sources/reddit": {
      remoteUrl: "https://cdn.example.com/plugins/reddit/remoteEntry.js", 
      type: "source",
      version: "1.1.0",
    },
    "@sources/rss": {
      remoteUrl: "https://cdn.example.com/plugins/rss/remoteEntry.js",
      type: "source", 
      version: "2.0.0",
    },
    "@transformers/content-normalizer": {
      remoteUrl: "https://cdn.example.com/plugins/content-normalizer/remoteEntry.js",
      type: "transformer",
      version: "1.0.0",
    },
    "@transformers/sentiment-analyzer": {
      remoteUrl: "https://cdn.example.com/plugins/sentiment-analyzer/remoteEntry.js",
      type: "transformer", 
      version: "1.3.0",
    },
    "@transformers/category-classifier": {
      remoteUrl: "https://cdn.example.com/plugins/category-classifier/remoteEntry.js",
      type: "transformer",
      version: "1.1.0",
    },
    "@transformers/duplicate-detector": {
      remoteUrl: "https://cdn.example.com/plugins/duplicate-detector/remoteEntry.js",
      type: "transformer",
      version: "1.0.0",
    },
  },
  secrets: {
    TWITTER_API_KEY: process.env.TWITTER_API_KEY!,
    REDDIT_CLIENT_ID: process.env.REDDIT_CLIENT_ID!,
    REDDIT_CLIENT_SECRET: process.env.REDDIT_CLIENT_SECRET!,
    OPENAI_API_KEY: process.env.OPENAI_API_KEY!, // For sentiment analysis
  },
});
```

## Data Collection Phase

Collect data from multiple sources in parallel:

```typescript
const collectData = Effect.gen(function* () {
  const pluginRuntime = yield* PluginRuntime;
  
  // Initialize all source plugins
  const twitterPlugin = yield* pluginRuntime.usePlugin("@sources/twitter", {
    secrets: {
      apiKey: "{{TWITTER_API_KEY}}",
    },
    variables: {
      timeout: 30000,
    },
  });
  
  const redditPlugin = yield* pluginRuntime.usePlugin("@sources/reddit", {
    secrets: {
      clientId: "{{REDDIT_CLIENT_ID}}",
      clientSecret: "{{REDDIT_CLIENT_SECRET}}",
    },
    variables: {
      timeout: 30000,
    },
  });
  
  const rssPlugin = yield* pluginRuntime.usePlugin("@sources/rss", {
    variables: {
      timeout: 15000,
      maxFeeds: 10,
    },
  });
  
  // Collect data from all sources in parallel
  const [twitterData, redditData, rssData] = yield* Effect.all([
    pluginRuntime.executePlugin(twitterPlugin, {
      procedure: "search",
      input: { 
        query: "artificial intelligence",
        limit: 50,
      },
      state: null,
    }),
    
    pluginRuntime.executePlugin(redditPlugin, {
      procedure: "search", 
      input: {
        query: "artificial intelligence",
        subreddits: ["MachineLearning", "artificial", "singularity"],
        limit: 30,
      },
      state: null,
    }),
    
    pluginRuntime.executePlugin(rssPlugin, {
      procedure: "fetchFeeds",
      input: {
        feeds: [
          "https://feeds.feedburner.com/oreilly/radar",
          "https://machinelearningmastery.com/feed/",
          "https://distill.pub/rss.xml",
        ],
        limit: 20,
      },
      state: null,
    }),
  ]);
  
  // Combine all items
  const allItems = [
    ...twitterData.items,
    ...redditData.items, 
    ...rssData.items,
  ];
  
  console.log(`Collected ${allItems.length} items from all sources`);
  return allItems;
});
```

## Data Processing Pipeline

Process the collected data through multiple transformation stages:

```typescript
const processData = (rawItems: any[]) => Effect.gen(function* () {
  const pluginRuntime = yield* PluginRuntime;
  
  // Initialize transformer plugins
  const normalizer = yield* pluginRuntime.usePlugin("@transformers/content-normalizer", {
    variables: {
      outputFormat: "standard",
      preserveMetadata: true,
    },
  });
  
  const sentimentAnalyzer = yield* pluginRuntime.usePlugin("@transformers/sentiment-analyzer", {
    secrets: {
      openaiApiKey: "{{OPENAI_API_KEY}}",
    },
    variables: {
      model: "gpt-3.5-turbo",
      batchSize: 10,
    },
  });
  
  const categoryClassifier = yield* pluginRuntime.usePlugin("@transformers/category-classifier", {
    variables: {
      categories: ["technology", "business", "science", "politics", "other"],
      confidenceThreshold: 0.7,
    },
  });
  
  const duplicateDetector = yield* pluginRuntime.usePlugin("@transformers/duplicate-detector", {
    variables: {
      similarityThreshold: 0.85,
      algorithm: "cosine",
    },
  });
  
  // Stage 1: Normalize content format
  console.log("Stage 1: Normalizing content...");
  const normalizedResult = yield* pluginRuntime.executePlugin(normalizer, {
    items: rawItems,
  });
  
  console.log(`Normalized ${normalizedResult.items.length} items`);
  
  // Stage 2: Analyze sentiment
  console.log("Stage 2: Analyzing sentiment...");
  const sentimentResult = yield* pluginRuntime.executePlugin(sentimentAnalyzer, {
    items: normalizedResult.items,
  });
  
  console.log(`Analyzed sentiment for ${sentimentResult.items.length} items`);
  
  // Stage 3: Classify categories
  console.log("Stage 3: Classifying categories...");
  const categoryResult = yield* pluginRuntime.executePlugin(categoryClassifier, {
    items: sentimentResult.items,
  });
  
  console.log(`Classified ${categoryResult.items.length} items`);
  
  // Stage 4: Remove duplicates
  console.log("Stage 4: Detecting duplicates...");
  const deduplicatedResult = yield* pluginRuntime.executePlugin(duplicateDetector, {
    items: categoryResult.items,
  });
  
  console.log(`Final result: ${deduplicatedResult.items.length} unique items`);
  
  return deduplicatedResult.items;
});
```

## Complete Pipeline

Combine collection and processing into a complete pipeline:

```typescript
const runAggregationPipeline = Effect.gen(function* () {
  console.log("Starting data aggregation pipeline...");
  
  // Phase 1: Data Collection
  const rawItems = yield* collectData;
  
  // Phase 2: Data Processing
  const processedItems = yield* processData(rawItems);
  
  // Phase 3: Generate Summary
  const summary = {
    totalItems: processedItems.length,
    bySource: processedItems.reduce((acc, item) => {
      acc[item.source] = (acc[item.source] || 0) + 1;
      return acc;
    }, {} as Record<string, number>),
    byCategory: processedItems.reduce((acc, item) => {
      acc[item.category] = (acc[item.category] || 0) + 1;
      return acc;
    }, {} as Record<string, number>),
    bySentiment: processedItems.reduce((acc, item) => {
      acc[item.sentiment] = (acc[item.sentiment] || 0) + 1;
      return acc;
    }, {} as Record<string, number>),
    averageScore: processedItems.reduce((sum, item) => sum + item.score, 0) / processedItems.length,
  };
  
  return {
    items: processedItems,
    summary,
    processedAt: new Date().toISOString(),
  };
});

// Execute the pipeline
const result = await runtime.runPromise(runAggregationPipeline);

console.log("Pipeline completed successfully!");
console.log("Summary:", result.summary);

// Clean up
await runtime.disposeRuntime();
```

## Error Handling

Add robust error handling to the pipeline:

```typescript
const resilientPipeline = Effect.gen(function* () {
  const pluginRuntime = yield* PluginRuntime;
  
  // Collect data with fallbacks
  const collectWithFallbacks = Effect.gen(function* () {
    const results = yield* Effect.allSettled([
      collectFromTwitter(),
      collectFromReddit(), 
      collectFromRSS(),
    ]);
    
    const successfulResults = results
      .filter((result): result is { _tag: "Right"; right: any[] } => 
        result._tag === "Right"
      )
      .map(result => result.right);
    
    const failedSources = results
      .filter(result => result._tag === "Left")
      .length;
    
    if (failedSources > 0) {
      console.warn(`${failedSources} data sources failed, continuing with available data`);
    }
    
    return successfulResults.flat();
  });
  
  // Process with error recovery
  const processWithRecovery = (items: any[]) => Effect.gen(function* () {
    const stages = [
      { name: "normalize", plugin: "@transformers/content-normalizer" },
      { name: "sentiment", plugin: "@transformers/sentiment-analyzer" },
      { name: "classify", plugin: "@transformers/category-classifier" },
      { name: "deduplicate", plugin: "@transformers/duplicate-detector" },
    ];
    
    let currentItems = items;
    
    for (const stage of stages) {
      try {
        const plugin = yield* pluginRuntime.usePlugin(stage.plugin, getStageConfig(stage.name));
        const result = yield* pluginRuntime.executePlugin(plugin, {
          items: currentItems,
        });
        currentItems = result.items;
        console.log(`${stage.name} stage: ${currentItems.length} items`);
      } catch (error) {
        console.error(`${stage.name} stage failed:`, error);
        // Continue with previous stage results
      }
    }
    
    return currentItems;
  });
  
  const rawItems = yield* collectWithFallbacks;
  const processedItems = yield* processWithRecovery(rawItems);
  
  return {
    items: processedItems,
    summary: generateSummary(processedItems),
    processedAt: new Date().toISOString(),
  };
}).pipe(
  Effect.catchAll((error) => {
    console.error("Pipeline failed:", error);
    return Effect.succeed({
      items: [],
      summary: { error: "Pipeline failed", totalItems: 0 },
      processedAt: new Date().toISOString(),
    });
  })
);
```

## Batch Processing

Handle large datasets efficiently:

```typescript
const batchProcessPipeline = Effect.gen(function* () {
  const pluginRuntime = yield* PluginRuntime;
  const BATCH_SIZE = 100;
  
  // Collect all data
  const allItems = yield* collectData;
  
  // Process in batches
  const batches = [];
  for (let i = 0; i < allItems.length; i += BATCH_SIZE) {
    batches.push(allItems.slice(i, i + BATCH_SIZE));
  }
  
  console.log(`Processing ${allItems.length} items in ${batches.length} batches`);
  
  const processedBatches = yield* Effect.all(
    batches.map((batch, index) =>
      Effect.gen(function* () {
        console.log(`Processing batch ${index + 1}/${batches.length}`);
        return yield* processData(batch);
      }).pipe(
        Effect.retry({ times: 2 }), // Retry failed batches
        Effect.catchAll((error) => {
          console.error(`Batch ${index + 1} failed:`, error);
          return Effect.succeed([]); // Return empty array for failed batches
        })
      )
    ),
    { concurrency: 3 } // Process 3 batches concurrently
  );
  
  const allProcessedItems = processedBatches.flat();
  
  return {
    items: allProcessedItems,
    summary: generateSummary(allProcessedItems),
    batchInfo: {
      totalBatches: batches.length,
      batchSize: BATCH_SIZE,
      successfulBatches: processedBatches.filter(batch => batch.length > 0).length,
    },
    processedAt: new Date().toISOString(),
  };
});
```

## Monitoring and Metrics

Add monitoring to track pipeline performance:

```typescript
interface PipelineMetrics {
  startTime: number;
  endTime?: number;
  duration?: number;
  itemsProcessed: number;
  stageMetrics: Record<string, {
    duration: number;
    itemsIn: number;
    itemsOut: number;
    errors: number;
  }>;
}

const monitoredPipeline = Effect.gen(function* () {
  const metrics: PipelineMetrics = {
    startTime: Date.now(),
    itemsProcessed: 0,
    stageMetrics: {},
  };
  
  const trackStage = <T>(stageName: string, operation: Effect.Effect<T, any>) =>
    Effect.gen(function* () {
      const stageStart = Date.now();
      let itemsIn = 0;
      let itemsOut = 0;
      let errors = 0;
      
      try {
        const result = yield* operation;
        itemsOut = Array.isArray(result) ? result.length : 
                   result && typeof result === 'object' && 'items' in result ? 
                   (result as any).items.length : 0;
        return result;
      } catch (error) {
        errors++;
        throw error;
      } finally {
        metrics.stageMetrics[stageName] = {
          duration: Date.now() - stageStart,
          itemsIn,
          itemsOut,
          errors,
        };
      }
    });
  
  // Execute pipeline with monitoring
  const rawItems = yield* trackStage("collection", collectData);
  const processedItems = yield* trackStage("processing", processData(rawItems));
  
  metrics.endTime = Date.now();
  metrics.duration = metrics.endTime - metrics.startTime;
  metrics.itemsProcessed = processedItems.length;
  
  return {
    items: processedItems,
    summary: generateSummary(processedItems),
    metrics,
    processedAt: new Date().toISOString(),
  };
});
```

## Usage Example

Complete example with configuration:

```typescript
async function runNewsAggregation() {
  const runtime = createPluginRuntime({
    registry: {
      // ... plugin registry
    },
    secrets: {
      // ... secrets
    },
  });
  
  try {
    const result = await runtime.runPromise(
      monitoredPipeline.pipe(
        Effect.timeout("5 minutes"), // Set overall timeout
        Effect.retry({ times: 1 }) // Retry once on failure
      )
    );
    
    // Save results
    await saveResults(result);
    
    // Log metrics
    console.log("Pipeline Metrics:");
    console.log(`Total duration: ${result.metrics.duration}ms`);
    console.log(`Items processed: ${result.metrics.itemsProcessed}`);
    console.log("Stage breakdown:", result.metrics.stageMetrics);
    
    return result;
  } catch (error) {
    console.error("Pipeline failed:", error);
    throw error;
  } finally {
    await runtime.disposeRuntime();
  }
}

// Helper functions
function getStageConfig(stageName: string) {
  const configs = {
    normalize: { variables: { outputFormat: "standard" } },
    sentiment: { 
      secrets: { openaiApiKey: "{{OPENAI_API_KEY}}" },
      variables: { model: "gpt-3.5-turbo" }
    },
    classify: { 
      variables: { 
        categories: ["tech", "business", "science", "politics", "other"] 
      }
    },
    deduplicate: { 
      variables: { similarityThreshold: 0.85 }
    },
  };
  return configs[stageName as keyof typeof configs] || {};
}

function generateSummary(items: any[]) {
  return {
    totalItems: items.length,
    bySource: items.reduce((acc, item) => {
      acc[item.source] = (acc[item.source] || 0) + 1;
      return acc;
    }, {} as Record<string, number>),
    byCategory: items.reduce((acc, item) => {
      acc[item.category] = (acc[item.category] || 0) + 1;
      return acc;
    }, {} as Record<string, number>),
    averageScore: items.length > 0 ? 
      items.reduce((sum, item) => sum + (item.score || 0), 0) / items.length : 0,
  };
}

async function saveResults(result: any) {
  // Save to database, file, or external service
  console.log(`Saving ${result.items.length} processed items...`);
}

// Run the aggregation
runNewsAggregation()
  .then(result => console.log("Aggregation completed:", result.summary))
  .catch(error => console.error("Aggregation failed:", error));
```

## Next Steps

- [Real-time Processing](./real-time-processing) - Build streaming data pipelines
- [Multi-source Pipeline](./multi-source-pipeline) - Advanced pipeline patterns
- [Runtime API](../runtime/api-reference) - Complete runtime reference
